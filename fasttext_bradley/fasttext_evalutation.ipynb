{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitd0f716ec3ba9485fbb6e044caca2f77a",
   "display_name": "Python 3.8.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import fasttext\n",
    "import fasttext_models as fm"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First thing's first, we need to make our models.\n",
      "We will be building 10 models in total: 5 will be trained on cleaned and lemmatized data, 5 will be trained on the raw text. Each of the 5 for those two training sets will vary by vector length: 25, 50, 100, 200, and 300.\n",
      "reviews_train.json loaded:\n",
      "   Id  HelpfulnessNumerator  HelpfulnessDenominator  Score  \\\n",
      "0   1                     1                       1      5   \n",
      "1   2                     0                       0      1   \n",
      "2   3                     1                       1      4   \n",
      "3   4                     3                       3      2   \n",
      "4   5                     0                       0      5   \n",
      "\n",
      "                 Summary                                               Text  \\\n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
      "\n",
      "                                               clean  \n",
      "0  buy several vitality can dog food product find...  \n",
      "1  product arrive label jumbo salt peanut peanut ...  \n",
      "2  confection around century light pillowy citrus...  \n",
      "3  look secret ingredient robitussin believe find...  \n",
      "4  great taffy great price wide assortment yummy ...  \n",
      "format()\n",
      "\n",
      "Preprocessing the data:\n",
      "Preprocessed cleaned data:\n",
      "__label__5 buy several vitality can dog food product find good quality product look like stew process meat smell well labrador finicky appreciate product well\n",
      "__label__1 product arrive label jumbo salt peanut peanut actually small sized unsalted sure error vendor intend represent product jumbo\n",
      "__label__4 confection around century light pillowy citrus gelatin nuts case filbert cut tiny square liberally coat powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar \n",
      "unclean format()\n",
      "AHAHAHHAHAHAHHAHAHAHA\n",
      "Preprocessed uncleaned data:\n",
      "OHOHOOAOHAOHAOHOHHOOHOHHOHOHOOH\n",
      "Making tests\n",
      "Tests made.\n",
      "mc0: classifier trained on clean reviews, 10 epochs, vector-size 25.\n",
      "mc1: classifier trained on clean reviews, 10 epochs, vector-size 50.\n",
      "mc2: classifier trained on clean reviews, 10 epochs, vector-size 100.\n",
      "mc3: classifier trained on clean reviews, 10 epochs, vector-size 200.\n",
      "mc4: classifier trained on clean reviews, 10 epochs, vector-size 300.\n",
      "mu0: classifier trained on unclean reviews, 10 epochs, vector-size 25.\n",
      "mu1: classifier trained on unclean reviews, 10 epochs, vector-size 50.\n",
      "mu2: classifier trained on unclean reviews, 10 epochs, vector-size 100.\n",
      "mu3: classifier trained on unclean reviews, 10 epochs, vector-size 200.\n",
      "mu4: classifier trained on unclean reviews, 10 epochs, vector-size 300.\n",
      "Models made.\n",
      "0\n",
      "25\n",
      "50\n",
      "100\n",
      "200\n",
      "300\n",
      "Models saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"First thing's first, we need to make our models.\\nWe will be building 10 models in total: 5 will be trained on cleaned and lemmatized data, 5 will be trained on the raw text. Each of the 5 for those two training sets will vary by vector length: 25, 50, 100, 200, and 300.\")\n",
    "# execfile('fasttext_models.py') - Does not work in python 3, use:\n",
    "#exec(open(\"./fasttext_models.py\").read())\n",
    "\n",
    "mcs,mus = ([mc0,mc1,mc2,mc3,mc4],[mu0,mu1,mu2,mu3,mu4]) = fm.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nHere are the top 50 words for the model trained on cleaned data with D100:\n['>', '<', 'br', '</s>', 'like', 'good', 'taste', 'flavor', 'one', 'get', 'love', 'product', 'make', 'use', 'coffee', 'great', 'try', 'well', 'food', 'buy', 'tea', 'find', 'would', 'eat', 'dog', 'go', 'really', 'time', 'much', 'amazon', 'order', 'also', 'price', 'bag', 'cup', 'give', 'little', 'even', 'drink', 'say', 'think', 'store', 'day', 'cat', 'add', 'box', 'chocolate', 'treat', 'come', 'first']\n\nHere are the top 50 words for the model trained on uncleaned data with D100:\n['the', 'I', 'and', 'a', 'to', 'of', 'is', 'it', '</s>', 'for', 'in', 'this', 'that', 'my', 'with', 'have', 'but', 'are', 'was', 'not', 'you', '/><br', 'on', 'as', 'like', 'they', 'so', 'be', 'The', 'or', 'at', 'these', 'just', 'them', 'very', 'from', 'one', 'good', 'It', '\"I', 'has', 'can', 'taste', 'will', 'would', 'had', 'all', 'more', 'than', 'when']\nNota Bene: I cannot figure out how to remove some of these symbols. I am pretty sure the strategy I used in fasttext_models.py works, but it is still not working out. My theory is that some of these are subwords... but I doubt that it is that simple.\n"
     ]
    }
   ],
   "source": [
    "# load the models for evaluation\n",
    "'''\n",
    "mc0 = fasttext.load_model('data/fasttext_skipgram_cleaned_D25.bin')\n",
    "mc1 = fasttext.load_model('data/fasttext_skipgram_cleaned_D50.bin')\n",
    "mc2 = fasttext.load_model('data/fasttext_skipgram_cleaned_D100.bin')\n",
    "mc3 = fasttext.load_model('data/fasttext_skipgram_cleaned_D200.bin')\n",
    "mc4 = fasttext.load_model('data/fasttext_skipgram_cleaned_D300.bin')\n",
    "mcs = [mc0,mc1,mc2,mc3,mc4]\n",
    "\n",
    "mu0 = fasttext.load_model('data/fasttext_skipgram_uncleaned_D25.bin')\n",
    "mu1 = fasttext.load_model('data/fasttext_skipgram_uncleaned_D50.bin')\n",
    "mu2 = fasttext.load_model('data/fasttext_skipgram_uncleaned_D100.bin')\n",
    "mu3 = fasttext.load_model('data/fasttext_skipgram_uncleaned_D200.bin')\n",
    "mu4 = fasttext.load_model('data/fasttext_skipgram_uncleaned_D300.bin')\n",
    "mus = [mu0,mu1,mu2,mu3,mu4]\n",
    "'''\n",
    "\n",
    "print(\"\\nHere are the top 50 words for the model trained on cleaned data with D100:\")\n",
    "mc2_words = mc4.get_words()\n",
    "print(mc2_words[:50])\n",
    "print(\"\\nHere are the top 50 words for the model trained on uncleaned data with D100:\")\n",
    "mu2_words = mu2.get_words()\n",
    "print(mu2_words[:50])\n",
    "\n",
    "print(\"Nota Bene: I cannot figure out how to remove some of these symbols. I am pretty sure the strategy I used in fasttext_models.py works, but it is still not working out. My theory is that some of these are subwords... but I doubt that it is that simple.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Now let's compare our 10 different models' accuracy, precision, and recall.\nFirst, a quick note about precision and recall: The fasttext test() function calculates  precision and recall \"at k\" for our models. Note that these are not the usual definitions of precision and recall used in most discussion about binary classifiers. More precisely, precision at k is:\n      P@k = r / k,\n      the # of relevant labels r divided by the number of top predictions k.\nIn our case, P@1 will be either 1 or 0 in each test case, because the top guess either is or is not the correct label.\nFor our purposes, just considering precision and recall at 1 makes sense, because there is only one label we care about: the correct one. When k>1, our models can only get a value of either 1/k or 0 in each test case, making this a poor measure of accuracy.\nIt is not explicitly stated in the fasttext documentation, but it is reasonable to assume that P@1 will give the same output as we would get using the sci kit library's tools.\nSpecifically, it gives the same results as sklearn.metrics.precision_score(average='micro') would. This means that fasttext.test() is calculating metrics globally by counting the total true positives, false negatives and false positives across all the labels and producing one number for the precision and recall score. Unfortunately, there seems to be no way to change this calculation beyond varying the k value.\nTo calculate accuracy for comparision with our other models (with different embeddings), we need to use our own code. See the module get_accuracy.py for more informaiton.\n\n\tThe following dataframe compares our models' preformance on test data:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Train Data  Word Vector Size  Accuracy  Precision @1  Recall @1       N\n",
       "0      Clean                25  0.590187      0.590187   0.590187  113691\n",
       "1      Clean                50  0.590170      0.590170   0.590170  113691\n",
       "2      Clean               100  0.589792      0.589792   0.589792  113691\n",
       "3      Clean               200  0.589924      0.589924   0.589924  113691\n",
       "4      Clean               300  0.590056      0.590056   0.590056  113691\n",
       "5    Unclean                25  0.686580      0.686580   0.686580  113691\n",
       "6    Unclean                50  0.687029      0.687029   0.687029  113691\n",
       "7    Unclean               100  0.686712      0.686712   0.686712  113691\n",
       "8    Unclean               200  0.687082      0.687082   0.687082  113691\n",
       "9    Unclean               300  0.686835      0.686835   0.686835  113691"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Train Data</th>\n      <th>Word Vector Size</th>\n      <th>Accuracy</th>\n      <th>Precision @1</th>\n      <th>Recall @1</th>\n      <th>N</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Clean</td>\n      <td>25</td>\n      <td>0.590187</td>\n      <td>0.590187</td>\n      <td>0.590187</td>\n      <td>113691</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Clean</td>\n      <td>50</td>\n      <td>0.590170</td>\n      <td>0.590170</td>\n      <td>0.590170</td>\n      <td>113691</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Clean</td>\n      <td>100</td>\n      <td>0.589792</td>\n      <td>0.589792</td>\n      <td>0.589792</td>\n      <td>113691</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Clean</td>\n      <td>200</td>\n      <td>0.589924</td>\n      <td>0.589924</td>\n      <td>0.589924</td>\n      <td>113691</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Clean</td>\n      <td>300</td>\n      <td>0.590056</td>\n      <td>0.590056</td>\n      <td>0.590056</td>\n      <td>113691</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Unclean</td>\n      <td>25</td>\n      <td>0.686580</td>\n      <td>0.686580</td>\n      <td>0.686580</td>\n      <td>113691</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Unclean</td>\n      <td>50</td>\n      <td>0.687029</td>\n      <td>0.687029</td>\n      <td>0.687029</td>\n      <td>113691</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Unclean</td>\n      <td>100</td>\n      <td>0.686712</td>\n      <td>0.686712</td>\n      <td>0.686712</td>\n      <td>113691</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Unclean</td>\n      <td>200</td>\n      <td>0.687082</td>\n      <td>0.687082</td>\n      <td>0.687082</td>\n      <td>113691</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Unclean</td>\n      <td>300</td>\n      <td>0.686835</td>\n      <td>0.686835</td>\n      <td>0.686835</td>\n      <td>113691</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import get_accuracy\n",
    "\n",
    "print(\"Now let's compare our 10 different models' accuracy, precision, and recall.\")\n",
    "print(\"First, a quick note about precision and recall: The fasttext test() function calculates  precision and recall \\\"at k\\\" for our models. Note that these are not the usual definitions of precision and recall used in most discussion about binary classifiers. More precisely, precision at k is:\\n      P@k = r / k,\\n      the # of relevant labels r divided by the number of top predictions k.\")\n",
    "print(\"In our case, P@1 will be either 1 or 0 in each test case, because the top guess either is or is not the correct label.\\nFor our purposes, just considering precision and recall at 1 makes sense, because there is only one label we care about: the correct one. When k>1, our models can only get a value of either 1/k or 0 in each test case, making this a poor measure of accuracy.\\nIt is not explicitly stated in the fasttext documentation, but it is reasonable to assume that P@1 will give the same output as we would get using the sci kit library's tools.\\nSpecifically, it gives the same results as sklearn.metrics.precision_score(average='micro') would. This means that fasttext.test() is calculating metrics globally by counting the total true positives, false negatives and false positives across all the labels and producing one number for the precision and recall score. Unfortunately, there seems to be no way to change this calculation beyond varying the k value.\")\n",
    "print(\"To calculate accuracy for comparision with our other models (with different embeddings), we need to use our own code. See the module get_accuracy.py for more informaiton.\\n\\n\\tThe following dataframe compares our models' preformance on test data:\")\n",
    "\n",
    "cdata = []\n",
    "for m in mcs :\n",
    "    data = []\n",
    "    data.append('Clean')\n",
    "    data.append(m.get_dimension())\n",
    "    n,p,r = m.test('data/reviews_uncleaned.test', k=1)\n",
    "    data.append(get_accuracy.get_accuracy(m))\n",
    "    data.append(p)\n",
    "    data.append(r)\n",
    "    data.append(n)\n",
    "    cdata.append(data)\n",
    "df1 = pd.DataFrame(cdata,columns = ['Train Data','Word Vector Size','Accuracy','Precision @1','Recall @1','N'])\n",
    "\n",
    "udata = []\n",
    "for m in mus :\n",
    "    data = []\n",
    "    data.append('Unclean')\n",
    "    data.append(m.get_dimension())\n",
    "    n,p,r = m.test('data/reviews_uncleaned.test', k=1)\n",
    "    data.append(get_accuracy.get_accuracy(m))\n",
    "    data.append(p)\n",
    "    data.append(r)\n",
    "    data.append(n)\n",
    "    udata.append(data)\n",
    "df2 = pd.DataFrame(udata,columns = ['Train Data','Word Vector Size','Accuracy','Precision @1','Recall @1','N'])\n",
    "\n",
    "df_merged=pd.concat([df1,df2], ignore_index=True)\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's get some more stats about our models. We will use scikit-learn to get some statistics for comparing our models.\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
    "\n",
    "# get_accuracy.get_guesses(m, labels, reviews) can give us the information we need for our analysis\n",
    "# get_accuracy.get_guesses() : A function that will return the a 3tuple (listof guesses, listof test labels, listof reviews), all matching index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now let's get the RMSE and accuracy for each of our ten models:\n",
    "# That means getting the list of predictions for each model, then using scikit to measure RMSE and accuracy, and finally adding it to df_merged\n",
    "\n",
    "# First, we need to get the string list of labels and reviews for the testing dataset\n",
    "# To do this, run ev.get_accuracy() on an arbitrary model.\n",
    "a, str_labels, reviews = get_accuracy.get_accuracy(mc2)\n",
    "\n",
    "str_guesses = [] # will be a listof listof Ints\n",
    "\n",
    "for m in mcs :\n",
    "    g, l, r = get_accuracy.get_guesses(m, str_labels, reviews)\n",
    "    str_guesses.append(g)\n",
    "\n",
    "for m in mus :\n",
    "    g, l, r = get_accuracy.get_guesses(m, str_labels, reviews)\n",
    "    str_guesses.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guesses correspond as such: guesses[0] --> mc0, guesses[1] --> mc1, ... guesses[9] --> mu4\n",
    "# make all of the guesses and labels into ints (necessary for RMSE):\n",
    "convert = {\n",
    "    '__label__1' : 1,\n",
    "    '__label__2' : 2,\n",
    "    '__label__3' : 3,\n",
    "    '__label__4' : 4,\n",
    "    '__label__5' : 5,\n",
    "}\n",
    "\n",
    "guesses = []\n",
    "\n",
    "for inner in str_guesses :\n",
    "    temp = []\n",
    "    for j in inner :\n",
    "        temp.append(convert[j])\n",
    "    guesses.append(temp)\n",
    "    \n",
    "labels = []\n",
    "\n",
    "for l in str_labels :\n",
    "    labels.append(convert[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1.3172043876432558, 1.3189594286289186, 1.318499206663412, 1.3186259504764013, 1.318319076073167, 1.2753002906328834, 1.2709892639549203, 1.2741169027360137, 1.270196629475611, 1.2738752599324947]\n"
     ]
    }
   ],
   "source": [
    "# calculate RMSEs\n",
    "RMSEs = []\n",
    "\n",
    "i = 0\n",
    "for m in mcs :\n",
    "    r = mean_squared_error(labels, guesses[i], squared=False)\n",
    "    RMSEs.append(r)\n",
    "    i += 1\n",
    "\n",
    "for m in mus :\n",
    "    r = mean_squared_error(labels, guesses[i], squared=False)\n",
    "    RMSEs.append(r)\n",
    "    i += 1\n",
    "\n",
    "print(RMSEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Train Data  Word Vector Size  Accuracy  Precision @1  Recall @1       N  \\\n",
       "0      Clean                25  0.590187      0.590187   0.590187  113691   \n",
       "1      Clean                50  0.590170      0.590170   0.590170  113691   \n",
       "2      Clean               100  0.589792      0.589792   0.589792  113691   \n",
       "3      Clean               200  0.589924      0.589924   0.589924  113691   \n",
       "4      Clean               300  0.590056      0.590056   0.590056  113691   \n",
       "5    Unclean                25  0.686580      0.686580   0.686580  113691   \n",
       "6    Unclean                50  0.687029      0.687029   0.687029  113691   \n",
       "7    Unclean               100  0.686712      0.686712   0.686712  113691   \n",
       "8    Unclean               200  0.687082      0.687082   0.687082  113691   \n",
       "9    Unclean               300  0.686835      0.686835   0.686835  113691   \n",
       "\n",
       "       RMSE  \n",
       "0  1.317204  \n",
       "1  1.318959  \n",
       "2  1.318499  \n",
       "3  1.318626  \n",
       "4  1.318319  \n",
       "5  1.275300  \n",
       "6  1.270989  \n",
       "7  1.274117  \n",
       "8  1.270197  \n",
       "9  1.273875  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Train Data</th>\n      <th>Word Vector Size</th>\n      <th>Accuracy</th>\n      <th>Precision @1</th>\n      <th>Recall @1</th>\n      <th>N</th>\n      <th>RMSE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Clean</td>\n      <td>25</td>\n      <td>0.590187</td>\n      <td>0.590187</td>\n      <td>0.590187</td>\n      <td>113691</td>\n      <td>1.317204</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Clean</td>\n      <td>50</td>\n      <td>0.590170</td>\n      <td>0.590170</td>\n      <td>0.590170</td>\n      <td>113691</td>\n      <td>1.318959</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Clean</td>\n      <td>100</td>\n      <td>0.589792</td>\n      <td>0.589792</td>\n      <td>0.589792</td>\n      <td>113691</td>\n      <td>1.318499</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Clean</td>\n      <td>200</td>\n      <td>0.589924</td>\n      <td>0.589924</td>\n      <td>0.589924</td>\n      <td>113691</td>\n      <td>1.318626</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Clean</td>\n      <td>300</td>\n      <td>0.590056</td>\n      <td>0.590056</td>\n      <td>0.590056</td>\n      <td>113691</td>\n      <td>1.318319</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Unclean</td>\n      <td>25</td>\n      <td>0.686580</td>\n      <td>0.686580</td>\n      <td>0.686580</td>\n      <td>113691</td>\n      <td>1.275300</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Unclean</td>\n      <td>50</td>\n      <td>0.687029</td>\n      <td>0.687029</td>\n      <td>0.687029</td>\n      <td>113691</td>\n      <td>1.270989</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Unclean</td>\n      <td>100</td>\n      <td>0.686712</td>\n      <td>0.686712</td>\n      <td>0.686712</td>\n      <td>113691</td>\n      <td>1.274117</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Unclean</td>\n      <td>200</td>\n      <td>0.687082</td>\n      <td>0.687082</td>\n      <td>0.687082</td>\n      <td>113691</td>\n      <td>1.270197</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Unclean</td>\n      <td>300</td>\n      <td>0.686835</td>\n      <td>0.686835</td>\n      <td>0.686835</td>\n      <td>113691</td>\n      <td>1.273875</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# Now simply merge the RMSE data with the df_merged dataframe and we're done!\n",
    "df_merged['RMSE'] = RMSEs\n",
    "df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Now let's take a look at some of the vector embeddings.\nLet's look at the vector for \"chocolate\" in the cleaned-data model:\n\n[-0.01593476  0.09763412 -0.03379307  0.05999282 -0.08622997  0.08399839\n -0.04973645  0.08821716  0.13238464  0.05473518 -0.05669185  0.00199589\n -0.05042139  0.08592727  0.01320869 -0.00363729 -0.06829321 -0.1010453\n  0.00493081 -0.02913536  0.1875842   0.25373566  0.14334284  0.05230877\n -0.12404761 -0.12505235 -0.01709576 -0.09508679  0.02292475  0.07112854\n -0.01314331 -0.05340886  0.04346133  0.00161902  0.10733654 -0.05789496\n -0.03510344  0.00099835  0.02680125 -0.08761607  0.09337546  0.01593737\n -0.04629854 -0.04687629  0.09315459  0.076052   -0.07473697  0.07008194\n  0.04242377 -0.23148057  0.04395402  0.06255033  0.06598131 -0.04706314\n -0.07252772  0.06335913  0.16925715  0.12258145  0.1409662   0.04487699\n  0.08523075  0.17074719  0.14271969  0.04857241 -0.01685663  0.06836378\n  0.06888569  0.0767359   0.02995878 -0.06690644 -0.02951036 -0.02265557\n -0.00402866  0.05403507 -0.03901019 -0.09827281  0.04990543  0.10013912\n  0.00276324 -0.07294284 -0.27236897  0.0230223  -0.03832762 -0.03091252\n -0.02132159  0.10480809  0.00500384  0.02353621  0.1285675  -0.10318157\n -0.08431382  0.00801327  0.09158476  0.0713802   0.01504861  0.06332015\n  0.08857583  0.01086797  0.05663011  0.10077764]\n\nWe can also look at the most similar words, or the vector's 'neighbors.' These are determined by the cosine similarity of vectors.\n[(0.8836485743522644, 'brand'), (0.8769121170043945, 'flour'), (0.84535813331604, 'simply'), (0.8214857578277588, 'review'), (0.816563069820404, 'order'), (0.8130179047584534, 'sit'), (0.8109937906265259, 'market'), (0.7957980632781982, 'worth'), (0.7954994440078735, 'blue'), (0.7743688225746155, 'believe')]\n\nFasttext also lets us try out analogies. Let's see how it does with the following:\n\n\"'hot' is to 'cold', what 'good' is to _____\"\n[(0.9265485405921936, 'really'), (0.9051209092140198, 'cook'), (0.8839849233627319, 'along'), (0.8761577010154724, 'honey'), (0.8655315041542053, 'delivery'), (0.8478816747665405, 'also'), (0.8394657373428345, 'set'), (0.8309633731842041, 'light'), (0.8261004686355591, '100'), (0.8203904032707214, 'puppy')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Now let's take a look at some of the vector embeddings.\")\n",
    "print(\"Let's look at the vector for \\\"chocolate\\\" in the cleaned-data model:\\n\")\n",
    "print(mc2.get_word_vector('chocolate'))\n",
    "print(\"\\nWe can also look at the most similar words, or the vector's 'neighbors.' These are determined by the cosine similarity of vectors.\")\n",
    "print(mc2.get_nearest_neighbors('chocolate'))\n",
    "\n",
    "print(\"\\nFasttext also lets us try out analogies. Let's see how it does with the following:\")\n",
    "print(\"\\n\\\"\\'hot\\' is to \\'cold\\', what \\'good\\' is to _____\\\"\")\n",
    "print(mc2.get_analogies('hot','cold','good'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}