{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitd0f716ec3ba9485fbb6e044caca2f77a",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import pandas as pd\n",
    "import fasttext\n",
    "import fasttext_models\n",
    "\n",
    "\n",
    "print(\"First thing's first, we need to make our models.\\nWe will be building 10 models in total: 5 will be trained on cleaned and lemmatized data, 5 will be trained on the raw text. Each of the 5 for those two training sets will vary by vector length: 25, 50, 100, 200, and 300.\")\n",
    "# execfile('fasttext_models.py') - Does not work python 3\n",
    "exec(open(\"./fasttext_models.py\").read())"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First thing's first, we need to make our models.\n",
      "We will be building 10 models in total: 5 will be trained on cleaned and lemmatized data, 5 will be trained on the raw text. Each of the 5 for those two training sets will vary by vector length: 25, 50, 100, 200, and 300.\n",
      "reviews_train.json loaded:\n",
      "   Id  HelpfulnessNumerator  HelpfulnessDenominator  Score  \\\n",
      "0   1                     1                       1      5   \n",
      "1   2                     0                       0      1   \n",
      "2   3                     1                       1      4   \n",
      "3   4                     3                       3      2   \n",
      "4   5                     0                       0      5   \n",
      "\n",
      "                 Summary                                               Text  \\\n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
      "\n",
      "                                               clean  \n",
      "0  buy several vitality can dog food product find...  \n",
      "1  product arrive label jumbo salt peanut peanut ...  \n",
      "2  confection around century light pillowy citrus...  \n",
      "3  look secret ingredient robitussin believe find...  \n",
      "4  great taffy great price wide assortment yummy ...  \n",
      "format()\n",
      "\n",
      "Preprocessing the data:\n",
      "Preprocessed cleaned data:\n",
      "__label__5 buy several vitality can dog food product find good quality product look like stew process meat smell well labrador finicky appreciate product well\n",
      "__label__1 product arrive label jumbo salt peanut peanut actually small sized unsalted sure error vendor intend represent product jumbo\n",
      "__label__4 confection around century light pillowy citrus gelatin nuts case filbert cut tiny square liberally coat powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar \n",
      "unclean format()\n",
      "Preprocessed uncleaned data:\n",
      "__label__5 I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
      "__label__1 \"Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"\"Jumbo\"\".\"\n",
      "__label__4 \"This is\n",
      "Making tests\n",
      "Tests made.\n",
      "mc0: classifier trained on clean reviews, 10 epochs, vector-size 25.\n",
      "mc1: classifier trained on clean reviews, 10 epochs, vector-size 50.\n",
      "mc2: classifier trained on clean reviews, 10 epochs, vector-size 100.\n",
      "mc3: classifier trained on clean reviews, 10 epochs, vector-size 200.\n",
      "mc4: classifier trained on clean reviews, 10 epochs, vector-size 200.\n",
      "mu0: classifier trained on unclean reviews, 10 epochs, vector-size 25.\n",
      "mu1: classifier trained on unclean reviews, 10 epochs, vector-size 50.\n",
      "mu2: classifier trained on unclean reviews, 10 epochs, vector-size 100.\n",
      "mu3: classifier trained on unclean reviews, 10 epochs, vector-size 200.\n",
      "mu4: classifier trained on unclean reviews, 10 epochs, vector-size 200.\n",
      "Models made.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nHere are the top 50 words for the model trained on cleaned data with D100:\n['>', '<', 'br', '</s>', 'like', 'good', 'taste', 'flavor', 'one', 'get', 'love', 'product', 'make', 'use', 'coffee', 'great', 'try', 'well', 'food', 'buy', 'tea', 'find', 'would', 'eat', 'dog', 'go', 'really', 'time', 'much', 'amazon', 'order', 'also', 'price', 'bag', 'cup', 'give', 'little', 'even', 'drink', 'say', 'think', 'store', 'day', 'cat', 'add', 'box', 'chocolate', 'treat', 'come', 'first']\n\nHere are the top 50 words for the model trained on uncleaned data with D100:\n['the', 'I', 'and', 'a', 'to', 'of', 'is', 'it', '</s>', 'for', 'in', 'this', 'that', 'my', 'with', 'have', 'but', 'are', 'was', 'not', 'you', '/><br', 'on', 'as', 'like', 'they', 'so', 'be', 'The', 'or', 'at', 'these', 'just', 'them', 'very', 'from', 'one', 'good', 'It', '\"I', 'has', 'can', 'taste', 'will', 'would', 'had', 'all', 'more', 'than', 'when']\n"
     ]
    }
   ],
   "source": [
    "# load the models for evaluation\n",
    "mc0 = fasttext.load_model('fasttext_skipgram_cleaned_D25.bin')\n",
    "mc1 = fasttext.load_model('fasttext_skipgram_cleaned_D50.bin')\n",
    "mc2 = fasttext.load_model('fasttext_skipgram_cleaned_D100.bin')\n",
    "mc3 = fasttext.load_model('fasttext_skipgram_cleaned_D200.bin')\n",
    "mc4 = fasttext.load_model('fasttext_skipgram_cleaned_D300.bin')\n",
    "\n",
    "mu0 = fasttext.load_model('fasttext_skipgram_uncleaned_D25.bin')\n",
    "mu1 = fasttext.load_model('fasttext_skipgram_uncleaned_D50.bin')\n",
    "mu2 = fasttext.load_model('fasttext_skipgram_uncleaned_D100.bin')\n",
    "mu3 = fasttext.load_model('fasttext_skipgram_uncleaned_D200.bin')\n",
    "mu4 = fasttext.load_model('fasttext_skipgram_uncleaned_D300.bin')\n",
    "\n",
    "print(\"\\nHere are the top 50 words for the model trained on cleaned data with D100:\")\n",
    "mc2_words = mc2.get_words()\n",
    "print(mc2_words[:50])\n",
    "print(\"\\nHere are the top 50 words for the model trained on uncleaned data with D100:\")\n",
    "mu2_words = mu2.get_words()\n",
    "print(mu2_words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "We can also measure the accuracy of these classfiers by examining their precision and recall when running on a test set.\n",
      "We will continue to use mc2 and mu2, the models trained with dim=100 on cleaned and nucleaned data respectively.\n",
      "The fasttext test() function will calculate the precision and recall at k for our models. These are not the usual definitions of precision and recall used when talking about binary classifiers. More precisely,\n",
      "      P@k = r / k,\n",
      "   the # of relevant labels r divided by number of predictions being considered, k.\n",
      "The k value determines how many of the top predictions we consider, e.g. do we want to evaluate just the label with the highest rank outputted by the model (k=1), or the top 3 labels (k=3)?\n",
      "Further, prec@1 will either be 1 or 0 in each case, because the top guess either is or is not relevant.\n",
      "For our purposes, just considering precision and recall at 1 makes sense, because there is only one label we care about: the correct one. When k>1, we can only get a value of either 1/k and or 0.\n",
      "It is not explicitly stated, in the fasttext documentation, but it is reasonable to say that P@1 will give the same output as we would get using the sci kit library.\n",
      "sklearn.metrics.precision_score() with the parameter average set to 'micro'. This means that fasttext.test() is calculating metrics globally by counting the total true positives, false negatives and false positives across all the labels and producing one number.\n",
      "       Model       N  Precision  Recall\n",
      "0    Cleaned  113691        0.2     1.0\n",
      "1  Uncleaned  113691        0.2     1.0\n",
      "       Model       N  Precision    Recall\n",
      "0    Cleaned  113691   0.427989  0.855978\n",
      "1  Uncleaned  113691   0.426401  0.852803\n",
      "       Model       N  Precision    Recall\n",
      "0    Cleaned  113691   0.695745  0.695745\n",
      "1  Uncleaned  113691   0.686184  0.686184\n",
      "       Model       N  Precision    Recall\n",
      "0    Cleaned  113691   0.695745  0.695745\n",
      "1  Uncleaned  113691   0.686184  0.686184\n",
      "\n",
      "Though the two models are different, they have both have equal precision and recall. I believe this is because each doc has only one possible label.\n",
      "\n",
      "Therefore, the accuracy of these models is equivalent to their recall and precision. (Partners confirm this pls)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"We can also measure the accuracy of these classfiers by examining their precision and recall when running on a test set.\\nWe will continue to use mc2 and mu2, the models trained with dim=100 on cleaned and uncleaned data respectively.\")\n",
    "print(\"The fasttext test() function will calculate the precision and recall \\\"at k\\\" for our models. These are not the usual definitions of precision and recall used in most discussion about binary classifiers. More precisely precision at k is:\\n      P@k = r / k,\\n   the # of relevant labels r divided by the number of top predictions being considered k.\")\n",
    "print(\"The k value determines how many of the top predictions we consider, e.g. do we want to evaluate just the label with the highest rank outputted by the model (k=1), or the top 3 labels (k=3)?\\nFurther, prec@1 will either be 1 or 0 in each test case, because the top guess either is or is not the relevant label.\\nFor our purposes, just considering precision and recall at 1 makes sense, because there is only one label we care about: the correct one. When k>1, our models can only get a value of either 1/k or 0 in each test case, making them seem less accurate than they are.\\nIt is not explicitly stated in the fasttext documentation, but it is reasonable to say that P@1 will give the same output as we would get using the sci kit library's tools.\\nSpecifically, it gives the same results as sklearn.metrics.precision_score() with parameter 'average' set to 'micro' would. This means that fasttext.test() is calculating metrics globally by counting the total true positives, false negatives and false positives across all the labels and producing one number for the precision and recall score. Unfortunately, there seems to be no way to change this calculation beyond varying the k value.\")\n",
    "print(\"Now, to calculate accuracy for comparision with our other models (with different embeddings), we\")\n",
    "\n",
    "\n",
    "n0,p0,r0 = mc2.test('reviews_cleaned.test', k=5)\n",
    "n1,p1,r1 = mu2.test('reviews_uncleaned.test', k=5)\n",
    "d = {'Model':['Cleaned', 'Uncleaned'], 'N':[n0,n1], 'Precision':[p0,p1],'Recall':[r0,r1]}\n",
    "df = pd.DataFrame(data=d)\n",
    "print(df)\n",
    "\n",
    "n0,p0,r0 = mc2.test('reviews_cleaned.test', k=2)\n",
    "n1,p1,r1 = mu2.test('reviews_uncleaned.test', k=2)\n",
    "d = {'Model':['Cleaned', 'Uncleaned'], 'N':[n0,n1], 'Precision':[p0,p1],'Recall':[r0,r1]}\n",
    "df = pd.DataFrame(data=d)\n",
    "print(df)\n",
    "\n",
    "n0,p0,r0 = mc2.test('reviews_cleaned.test', k=1)\n",
    "n1,p1,r1 = mu2.test('reviews_uncleaned.test', k=1)\n",
    "d = {'Model':['Cleaned', 'Uncleaned'], 'N':[n0,n1], 'Precision':[p0,p1],'Recall':[r0,r1]}\n",
    "df = pd.DataFrame(data=d)\n",
    "print(df)\n",
    "\n",
    "n0,p0,r0 = mc2.test('reviews_cleaned.test')\n",
    "n1,p1,r1 = mu2.test('reviews_uncleaned.test')\n",
    "d = {'Model':['Cleaned', 'Uncleaned'], 'N':[n0,n1], 'Precision':[p0,p1],'Recall':[r0,r1]}\n",
    "df = pd.DataFrame(data=d)\n",
    "print(df)\n",
    "\n",
    "print(\"\\nThough the two models are different, they have both have equal precision and recall. I believe this is because each doc has only one possible label.\")\n",
    "\n",
    "print(\"\\nTherefore, the accuracy of these models is equivalent to their recall and precision. (Partners confirm this pls)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Now let's take a look at some of the vector embeddings.\nLet's look at the vector for \"chocolate\" in the cleaned-data model:\n\n[-5.34566976e-02  4.42042574e-02  2.61640791e-02  8.67447108e-02\n  5.03236912e-02  3.56525965e-02  3.60676199e-02  2.56518926e-02\n -2.39018742e-02  1.40339553e-01 -3.05654109e-03  6.27863109e-02\n  5.45005016e-02 -5.31435311e-02 -3.14662009e-02 -6.71180012e-03\n  6.32468378e-03 -6.69706985e-02  3.44938226e-02 -9.28740203e-03\n  5.87618770e-03  5.69513105e-02  1.77001916e-02 -1.93954408e-01\n -6.99649453e-02  6.87073776e-03  1.68667227e-01 -4.09761108e-02\n  2.34629195e-02  2.90166270e-02 -5.45262508e-02  2.42105592e-02\n  6.55639637e-03  1.02791995e-01 -8.29611719e-03  4.12083864e-02\n  3.33829001e-02  5.91321252e-02 -1.73410866e-02 -6.61506280e-02\n  2.06537601e-02  9.40880850e-02 -2.30859537e-02 -6.78514224e-03\n  3.00923660e-02 -3.93865556e-02  6.54292926e-02 -4.35254946e-02\n  1.31859416e-02 -8.57713353e-03  1.29323145e-02 -2.09649820e-02\n -2.06249151e-02  1.07680736e-02  4.72024865e-02  1.74392071e-02\n  5.86557807e-03  3.13244909e-02  3.13054351e-03  4.76724543e-02\n -3.05161700e-02  3.23871337e-02  2.29520667e-02  7.54164392e-03\n -6.83219284e-02  4.21607718e-02 -8.61037709e-03 -3.47585930e-03\n -7.73199275e-03 -4.75379713e-02 -2.77802926e-02 -5.54035790e-02\n  3.29770031e-03  1.92037895e-02 -6.12792384e-04 -7.15153292e-02\n  7.63666257e-03  5.81294578e-03  1.09731341e-02 -1.22665174e-01\n  8.69668741e-03 -4.02458608e-02 -3.17676105e-02 -1.01206480e-02\n -9.16370098e-03 -1.20939873e-03 -6.72436655e-02  6.90622479e-02\n -3.34300078e-03 -1.80824883e-02  6.82222005e-03 -9.30074044e-03\n  5.25999963e-02  2.40716450e-02 -5.23199476e-02 -5.94830662e-02\n -5.17457537e-02  1.19972769e-02 -1.97625272e-02 -2.22222824e-02\n -5.74463559e-03  2.26346180e-02 -5.80435134e-02  1.45667410e-02\n  1.89418085e-02 -3.15779857e-02 -4.93152887e-02  2.89972872e-02\n  6.40051579e-03 -1.40680417e-01  1.20830514e-01 -1.71092544e-02\n -3.60756442e-02  2.26770621e-02  6.98396983e-03 -3.83744878e-03\n  9.04294550e-02 -2.17840131e-02  8.21298510e-02 -2.25738063e-02\n  7.29061887e-02 -5.10484762e-02  2.98855361e-03  1.47362659e-02\n -8.96764323e-02 -2.64649317e-02  5.59552088e-02  4.42293065e-04\n -2.79938965e-03  3.53174992e-02  3.45636085e-02 -9.40772071e-02\n  1.00529781e-02  2.58140285e-02  3.47065069e-02  6.99041318e-03\n  6.39228791e-04  1.11228026e-01 -3.75243314e-02  6.08923659e-02\n  8.22262838e-02 -5.87962475e-03  7.30611151e-03 -5.67542315e-02\n -4.40711491e-02  6.20934740e-03  2.71177515e-02 -7.51998946e-02\n -5.84786125e-02 -6.48507327e-02 -3.64263244e-02 -1.93910033e-03\n -1.92254372e-02  4.39324453e-02  3.50633822e-02  2.49520130e-02\n -2.67630331e-02 -4.74987254e-02  1.24407746e-02 -5.50655834e-02\n  7.12439716e-02  6.66580796e-02 -2.79456880e-02 -6.43895790e-02\n -1.56759284e-02 -5.08908257e-02 -1.10663967e-02  2.22982857e-02\n -3.47243957e-02 -4.54387162e-03  1.32471463e-02 -4.34352309e-02\n  1.07654035e-02 -6.29606619e-02 -9.90921864e-04  4.15190049e-02\n -6.83614388e-02  1.96526535e-02  8.14130902e-03  2.62099244e-02\n  2.21341550e-02 -1.85056459e-02  6.03684690e-03 -3.95159014e-02\n  2.56711412e-02 -7.50907436e-02  3.54459807e-02  4.38930616e-02\n -2.35931985e-02 -4.77718972e-02  4.24719602e-02  1.04223436e-03\n  4.38899808e-02 -1.03020675e-01 -3.87576744e-02 -4.60859686e-02\n  2.51878537e-02 -3.34783364e-03 -9.67391655e-02 -6.57354668e-02\n  8.72486904e-02  3.47033739e-02 -2.69655399e-02  7.53831714e-02\n  5.35014495e-02  1.03805855e-01 -1.11914203e-02  5.73513918e-02\n -3.09887417e-02  2.94860583e-02 -6.39582723e-02 -2.05955245e-02\n  4.33079414e-02  1.88483223e-02  6.26382381e-02  1.55789973e-02\n  2.66208798e-02  2.45563150e-03  3.33438851e-02  4.77806889e-02\n -3.28802057e-02 -5.93861099e-03  2.32517179e-02 -2.77243229e-03\n  8.34938362e-02 -4.07135747e-02 -1.93018224e-02  1.16214775e-01\n  6.24350831e-02 -9.77454856e-02  1.15889953e-02 -1.02565765e-01\n  4.42359932e-02  7.06183240e-02 -8.69484246e-02  9.85827949e-03\n -6.92394227e-02 -2.95854229e-02  2.17713658e-02 -5.27621200e-03\n  3.92539091e-02  2.82156728e-02 -4.82844114e-02 -9.19189230e-02\n  1.85739771e-02  5.98561168e-02  3.35091427e-02 -2.53962353e-02\n  1.08581744e-02 -3.29737412e-03  8.07094648e-02 -2.73936484e-02\n  3.06547787e-02  4.96831946e-02 -5.46929166e-02  5.76814860e-02\n -2.50889435e-02  2.36757956e-02 -1.88972168e-02  3.63025852e-02\n  9.58333694e-05  2.01484356e-02 -7.66397643e-05 -5.68468124e-02\n -7.42105320e-02 -1.98630039e-02 -3.33800800e-02  1.53173814e-02\n -4.72347476e-02 -1.08464314e-02  2.24307906e-02 -7.64301568e-02\n -1.24534629e-01  1.04677072e-02  7.26704299e-02  1.76817626e-02\n  4.39898036e-02 -7.25514293e-02 -3.30427103e-02 -4.65908134e-03\n -6.20441325e-02 -1.84075125e-02 -3.46813314e-02  4.19948734e-02\n -1.69928577e-02  9.63525381e-03  8.75814632e-03  4.58990186e-02\n  6.63127936e-03  1.37696974e-02 -5.09885289e-02  3.23534980e-02\n  1.14396587e-02 -3.13981324e-02 -4.50248979e-02  2.87174131e-03\n -2.29608044e-02 -4.62151915e-02 -3.36816050e-02  1.51991500e-02]\n\nWe can also look at the most similar words, or the vector's 'neighbors.' These are determined by the cosine similarity of vectors.\n[(0.8961277008056641, 'flour'), (0.8831185102462769, 'brand'), (0.8444476127624512, 'simply'), (0.8386797904968262, 'order'), (0.8333169221878052, 'review'), (0.8227343559265137, 'sit'), (0.8104110360145569, 'sugar'), (0.8049927353858948, 'blue'), (0.7989788055419922, 'syrup'), (0.7955530285835266, 'market')]\n\nFasttext also lets us try out analogies. Let's see how it does with the following:\n\n\"'hot' is to 'cold', what 'good' is to _____\"\n[(0.9377390742301941, 'really'), (0.9368701577186584, 'cook'), (0.914268970489502, '100'), (0.9099522829055786, 'along'), (0.9018288254737854, 'honey'), (0.8907776474952698, 'delivery'), (0.8690614104270935, 'also'), (0.8633094429969788, 'rice'), (0.8567630648612976, 'drink'), (0.8527938723564148, 'sauce')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Now let's take a look at some of the vector embeddings.\")\n",
    "print(\"Let's look at the vector for \\\"chocolate\\\" in the cleaned-data model:\\n\")\n",
    "print(mc2.get_word_vector('chocolate'))\n",
    "print(\"\\nWe can also look at the most similar words, or the vector's 'neighbors.' These are determined by the cosine similarity of vectors.\")\n",
    "print(mc2.get_nearest_neighbors('chocolate'))\n",
    "\n",
    "print(\"\\nFasttext also lets us try out analogies. Let's see how it does with the following:\")\n",
    "print(\"\\n\\\"\\'hot\\' is to \\'cold\\', what \\'good\\' is to _____\\\"\")\n",
    "print(mc2.get_analogies('hot','cold','good'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WOWEEE!\n563\n563\n"
     ]
    }
   ],
   "source": [
    "print(\"WOWEEE!\")\n",
    "print(len(mc2.words))\n",
    "print(len(mc2.words))\n",
    "\n",
    "# maybe here we can try out some vector addition/subtraction to see what kind of meaning the model captures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}