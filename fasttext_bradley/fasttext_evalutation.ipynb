{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitd0f716ec3ba9485fbb6e044caca2f77a",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import fasttext\n",
    "import fasttext_models as fm"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First thing's first, we need to make our models.\n",
      "We will be building 10 models in total: 5 will be trained on cleaned and lemmatized data, 5 will be trained on the raw text. Each of the 5 for those two training sets will vary by vector length: 25, 50, 100, 200, and 300.\n",
      "reviews_train.json loaded:\n",
      "   Id  HelpfulnessNumerator  HelpfulnessDenominator  Score  \\\n",
      "0   1                     1                       1      5   \n",
      "1   2                     0                       0      1   \n",
      "2   3                     1                       1      4   \n",
      "3   4                     3                       3      2   \n",
      "4   5                     0                       0      5   \n",
      "\n",
      "                 Summary                                               Text  \\\n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
      "\n",
      "                                               clean  \n",
      "0  buy several vitality can dog food product find...  \n",
      "1  product arrive label jumbo salt peanut peanut ...  \n",
      "2  confection around century light pillowy citrus...  \n",
      "3  look secret ingredient robitussin believe find...  \n",
      "4  great taffy great price wide assortment yummy ...  \n",
      "format()\n",
      "\n",
      "Preprocessing the data:\n",
      "Preprocessed cleaned data:\n",
      "__label__5 buy several vitality can dog food product find good quality product look like stew process meat smell well labrador finicky appreciate product well\n",
      "__label__1 product arrive label jumbo salt peanut peanut actually small sized unsalted sure error vendor intend represent product jumbo\n",
      "__label__4 confection around century light pillowy citrus gelatin nuts case filbert cut tiny square liberally coat powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar \n",
      "unclean format()\n",
      "Preprocessed uncleaned data:\n",
      "__label__5 I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
      "__label__1 \"Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"\"Jumbo\"\".\"\n",
      "__label__4 \"This is\n",
      "Making tests\n",
      "Tests made.\n",
      "mc0: classifier trained on clean reviews, 10 epochs, vector-size 25.\n",
      "mc1: classifier trained on clean reviews, 10 epochs, vector-size 50.\n",
      "mc2: classifier trained on clean reviews, 10 epochs, vector-size 100.\n",
      "mc3: classifier trained on clean reviews, 10 epochs, vector-size 200.\n",
      "mc4: classifier trained on clean reviews, 10 epochs, vector-size 300.\n",
      "mu0: classifier trained on unclean reviews, 10 epochs, vector-size 25.\n",
      "mu1: classifier trained on unclean reviews, 10 epochs, vector-size 50.\n",
      "mu2: classifier trained on unclean reviews, 10 epochs, vector-size 100.\n",
      "mu3: classifier trained on unclean reviews, 10 epochs, vector-size 200.\n",
      "mu4: classifier trained on unclean reviews, 10 epochs, vector-size 300.\n",
      "Models made.\n",
      "0\n",
      "25\n",
      "50\n",
      "100\n",
      "200\n",
      "300\n",
      "Models saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"First thing's first, we need to make our models.\\nWe will be building 10 models in total: 5 will be trained on cleaned and lemmatized data, 5 will be trained on the raw text. Each of the 5 for those two training sets will vary by vector length: 25, 50, 100, 200, and 300.\")\n",
    "# execfile('fasttext_models.py') - Does not work in python 3, use:\n",
    "#exec(open(\"./fasttext_models.py\").read())\n",
    "\n",
    "mcs,mus = ([mc0,mc1,mc2,mc3,mc4],[mu0,mu1,mu2,mu3,mu4]) = fm.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nHere are the top 50 words for the model trained on cleaned data with D100:\n['>', '<', 'br', '</s>', 'like', 'good', 'taste', 'flavor', 'one', 'get', 'love', 'product', 'make', 'use', 'coffee', 'great', 'try', 'well', 'food', 'buy', 'tea', 'find', 'would', 'eat', 'dog', 'go', 'really', 'time', 'much', 'amazon', 'order', 'also', 'price', 'bag', 'cup', 'give', 'little', 'even', 'drink', 'say', 'think', 'store', 'day', 'cat', 'add', 'box', 'chocolate', 'treat', 'come', 'first']\n\nHere are the top 50 words for the model trained on uncleaned data with D100:\n['the', 'I', 'and', 'a', 'to', 'of', 'is', 'it', '</s>', 'for', 'in', 'this', 'that', 'my', 'with', 'have', 'but', 'are', 'was', 'not', 'you', '/><br', 'on', 'as', 'like', 'they', 'so', 'be', 'The', 'or', 'at', 'these', 'just', 'them', 'very', 'from', 'one', 'good', 'It', '\"I', 'has', 'can', 'taste', 'will', 'would', 'had', 'all', 'more', 'than', 'when']\nNota Bene: I cannot figure out how to remove some of these symbols. I am pretty sure the strategy I used in fasttext_models.py works, but it is still not working out. My theory is that some of these are subwords... but I doubt that it is that simple.\n"
     ]
    }
   ],
   "source": [
    "# load the models for evaluation\n",
    "'''\n",
    "mc0 = fasttext.load_model('data/fasttext_skipgram_cleaned_D25.bin')\n",
    "mc1 = fasttext.load_model('data/fasttext_skipgram_cleaned_D50.bin')\n",
    "mc2 = fasttext.load_model('data/fasttext_skipgram_cleaned_D100.bin')\n",
    "mc3 = fasttext.load_model('data/fasttext_skipgram_cleaned_D200.bin')\n",
    "mc4 = fasttext.load_model('data/fasttext_skipgram_cleaned_D300.bin')\n",
    "mcs = [mc0,mc1,mc2,mc3,mc4]\n",
    "\n",
    "for m in mcs :\n",
    "    print(m.get_dimension())\n",
    "mu0 = fasttext.load_model('data/fasttext_skipgram_uncleaned_D25.bin')\n",
    "mu1 = fasttext.load_model('data/fasttext_skipgram_uncleaned_D50.bin')\n",
    "mu2 = fasttext.load_model('data/fasttext_skipgram_uncleaned_D100.bin')\n",
    "mu3 = fasttext.load_model('data/fasttext_skipgram_uncleaned_D200.bin')\n",
    "mu4 = fasttext.load_model('data/fasttext_skipgram_uncleaned_D300.bin')\n",
    "mus = [mu0,mu1,mu2,mu3,mu4]\n",
    "'''\n",
    "\n",
    "print(\"\\nHere are the top 50 words for the model trained on cleaned data with D100:\")\n",
    "mc2_words = mc4.get_words()\n",
    "print(mc2_words[:50])\n",
    "print(\"\\nHere are the top 50 words for the model trained on uncleaned data with D100:\")\n",
    "mu2_words = mu2.get_words()\n",
    "print(mu2_words[:50])\n",
    "\n",
    "print(\"Nota Bene: I cannot figure out how to remove some of these symbols. I am pretty sure the strategy I used in fasttext_models.py works, but it is still not working out. My theory is that some of these are subwords... but I doubt that it is that simple.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Now let's compare our 10 different models' accuracy, precision, and recall.\nFirst, a quick note about precision and recall: The fasttext test() function calculates  precision and recall \"at k\" for our models. Note that these are not the usual definitions of precision and recall used in most discussion about binary classifiers. More precisely, precision at k is:\n      P@k = r / k,\n      the # of relevant labels r divided by the number of top predictions k.\nIn our case, P@1 will be either 1 or 0 in each test case, because the top guess either is or is not the correct label.\nFor our purposes, just considering precision and recall at 1 makes sense, because there is only one label we care about: the correct one. When k>1, our models can only get a value of either 1/k or 0 in each test case, making this a poor measure of accuracy.\nIt is not explicitly stated in the fasttext documentation, but it is reasonable to assume that P@1 will give the same output as we would get using the sci kit library's tools.\nSpecifically, it gives the same results as sklearn.metrics.precision_score(average='micro') would. This means that fasttext.test() is calculating metrics globally by counting the total true positives, false negatives and false positives across all the labels and producing one number for the precision and recall score. Unfortunately, there seems to be no way to change this calculation beyond varying the k value.\nTo calculate accuracy for comparision with our other models (with different embeddings), we need to use our own code. See the module get_accuracy.py for more informaiton.\n\n\tThe following dataframe compares our models' preformance on test data:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Train Data  Word Vector Size  Accuracy  Precision @1  Recall @1       N\n",
       "0      Clean                25  0.588402      0.588402   0.588402  113691\n",
       "1      Clean                50  0.590777      0.590777   0.590777  113691\n",
       "2      Clean               100  0.590407      0.590407   0.590407  113691\n",
       "3      Clean               200  0.587892      0.587892   0.587892  113691\n",
       "4      Clean               300  0.590680      0.590680   0.590680  113691\n",
       "5    Unclean                25  0.686396      0.686396   0.686396  113691\n",
       "6    Unclean                50  0.686774      0.686774   0.686774  113691\n",
       "7    Unclean               100  0.687038      0.687038   0.687038  113691\n",
       "8    Unclean               200  0.686774      0.686774   0.686774  113691\n",
       "9    Unclean               300  0.686906      0.686906   0.686906  113691"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Train Data</th>\n      <th>Word Vector Size</th>\n      <th>Accuracy</th>\n      <th>Precision @1</th>\n      <th>Recall @1</th>\n      <th>N</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Clean</td>\n      <td>25</td>\n      <td>0.588402</td>\n      <td>0.588402</td>\n      <td>0.588402</td>\n      <td>113691</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Clean</td>\n      <td>50</td>\n      <td>0.590777</td>\n      <td>0.590777</td>\n      <td>0.590777</td>\n      <td>113691</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Clean</td>\n      <td>100</td>\n      <td>0.590407</td>\n      <td>0.590407</td>\n      <td>0.590407</td>\n      <td>113691</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Clean</td>\n      <td>200</td>\n      <td>0.587892</td>\n      <td>0.587892</td>\n      <td>0.587892</td>\n      <td>113691</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Clean</td>\n      <td>300</td>\n      <td>0.590680</td>\n      <td>0.590680</td>\n      <td>0.590680</td>\n      <td>113691</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Unclean</td>\n      <td>25</td>\n      <td>0.686396</td>\n      <td>0.686396</td>\n      <td>0.686396</td>\n      <td>113691</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Unclean</td>\n      <td>50</td>\n      <td>0.686774</td>\n      <td>0.686774</td>\n      <td>0.686774</td>\n      <td>113691</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Unclean</td>\n      <td>100</td>\n      <td>0.687038</td>\n      <td>0.687038</td>\n      <td>0.687038</td>\n      <td>113691</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Unclean</td>\n      <td>200</td>\n      <td>0.686774</td>\n      <td>0.686774</td>\n      <td>0.686774</td>\n      <td>113691</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Unclean</td>\n      <td>300</td>\n      <td>0.686906</td>\n      <td>0.686906</td>\n      <td>0.686906</td>\n      <td>113691</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "import get_accuracy\n",
    "\n",
    "print(\"Now let's compare our 10 different models' accuracy, precision, and recall.\")\n",
    "print(\"First, a quick note about precision and recall: The fasttext test() function calculates  precision and recall \\\"at k\\\" for our models. Note that these are not the usual definitions of precision and recall used in most discussion about binary classifiers. More precisely, precision at k is:\\n      P@k = r / k,\\n      the # of relevant labels r divided by the number of top predictions k.\")\n",
    "print(\"In our case, P@1 will be either 1 or 0 in each test case, because the top guess either is or is not the correct label.\\nFor our purposes, just considering precision and recall at 1 makes sense, because there is only one label we care about: the correct one. When k>1, our models can only get a value of either 1/k or 0 in each test case, making this a poor measure of accuracy.\\nIt is not explicitly stated in the fasttext documentation, but it is reasonable to assume that P@1 will give the same output as we would get using the sci kit library's tools.\\nSpecifically, it gives the same results as sklearn.metrics.precision_score(average='micro') would. This means that fasttext.test() is calculating metrics globally by counting the total true positives, false negatives and false positives across all the labels and producing one number for the precision and recall score. Unfortunately, there seems to be no way to change this calculation beyond varying the k value.\")\n",
    "print(\"To calculate accuracy for comparision with our other models (with different embeddings), we need to use our own code. See the module get_accuracy.py for more informaiton.\\n\\n\\tThe following dataframe compares our models' preformance on test data:\")\n",
    "\n",
    "cdata = []\n",
    "for m in mcs :\n",
    "    data = []\n",
    "    data.append('Clean')\n",
    "    data.append(m.get_dimension())\n",
    "    n,p,r = m.test('data/reviews_uncleaned.test', k=1)\n",
    "    data.append(get_accuracy.get_accuracy(m))\n",
    "    data.append(p)\n",
    "    data.append(r)\n",
    "    data.append(n)\n",
    "    cdata.append(data)\n",
    "df1 = pd.DataFrame(cdata,columns = ['Train Data','Word Vector Size','Accuracy','Precision @1','Recall @1','N'])\n",
    "\n",
    "udata = []\n",
    "for m in mus :\n",
    "    data = []\n",
    "    data.append('Unclean')\n",
    "    data.append(m.get_dimension())\n",
    "    n,p,r = m.test('data/reviews_uncleaned.test', k=1)\n",
    "    data.append(get_accuracy.get_accuracy(m))\n",
    "    data.append(p)\n",
    "    data.append(r)\n",
    "    data.append(n)\n",
    "    udata.append(data)\n",
    "df2 = pd.DataFrame(udata,columns = ['Train Data','Word Vector Size','Accuracy','Precision @1','Recall @1','N'])\n",
    "\n",
    "df_merged=pd.concat([df1,df2], ignore_index=True)\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Now let's take a look at some of the vector embeddings.\nLet's look at the vector for \"chocolate\" in the cleaned-data model:\n\n[-9.51359496e-02 -4.76726629e-02  1.50278946e-02  6.76411688e-02\n -2.81996746e-02 -2.65564267e-02  3.68301980e-02  1.05108298e-01\n  2.63368885e-04 -6.05497696e-02 -7.75734782e-02 -3.38424481e-02\n  3.83751877e-02 -2.14667842e-02  1.07903354e-01 -8.65911245e-02\n -8.62116069e-02 -4.47578393e-02 -7.97315761e-02 -1.80238821e-02\n  1.65816829e-01  3.74060690e-01  4.61086817e-02  6.00010864e-02\n -1.17741935e-01 -7.76912197e-02 -2.82375310e-02 -1.13792568e-01\n -2.93733720e-02  5.69721535e-02  2.53809225e-02  1.08619863e-02\n  5.20594679e-02 -6.57318346e-03  9.95274186e-02 -1.51301891e-01\n -5.19979559e-02 -2.85977349e-02  3.99339199e-02 -8.51704627e-02\n -2.39323429e-03  4.83661629e-02 -5.15437946e-02 -1.06330663e-02\n  8.36717039e-02  6.82203993e-02  3.81557643e-02  2.58158278e-02\n  4.78198193e-02 -9.13953558e-02  5.77312782e-02  4.75437790e-02\n  7.45898709e-02 -4.86135436e-03 -8.15389454e-02  2.16839090e-02\n  1.49944440e-01  6.95124343e-02  5.95881268e-02 -2.56650932e-02\n  9.53272805e-02  1.13294177e-01  1.38722152e-01  1.94489770e-02\n -3.70758064e-02  8.00946951e-02  8.34661499e-02  7.40024373e-02\n  2.07211562e-02 -7.29830265e-02 -6.72446156e-04  2.32497579e-03\n -1.03153670e-02  5.61359376e-02 -5.07987291e-02 -5.32918908e-02\n  1.11146130e-01  1.10647731e-01  6.59762546e-02 -5.65684587e-02\n -2.45668262e-01  3.89493927e-02 -3.95254679e-02 -5.52529879e-02\n  2.33489517e-02  8.32260400e-02  1.34106632e-02  1.13234883e-02\n  1.26461908e-01 -1.57468811e-01 -1.21448681e-01 -2.51551289e-02\n  1.59277290e-01  8.21279585e-02 -6.61564665e-03  2.30108630e-02\n  6.31690025e-02  1.62979450e-05  3.98378000e-02  8.38910490e-02]\n\nWe can also look at the most similar words, or the vector's 'neighbors.' These are determined by the cosine similarity of vectors.\n[(0.8983187079429626, 'flour'), (0.8901064395904541, 'brand'), (0.8524872064590454, 'order'), (0.8509114384651184, 'simply'), (0.8473103642463684, 'coconut'), (0.8368926644325256, 'sit'), (0.8262587189674377, 'review'), (0.81158447265625, 'syrup'), (0.8093804121017456, 'blue'), (0.8078750967979431, 'market')]\n\nFasttext also lets us try out analogies. Let's see how it does with the following:\n\n\"'hot' is to 'cold', what 'good' is to _____\"\n[(0.9346034526824951, 'cook'), (0.8978717923164368, '100'), (0.8970887660980225, 'really'), (0.8862702250480652, 'along'), (0.8786314129829407, 'honey'), (0.8591097593307495, 'delivery'), (0.8492725491523743, 'rice'), (0.8390302062034607, 'set'), (0.8326708674430847, 'also'), (0.8282776474952698, 'difference')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Now let's take a look at some of the vector embeddings.\")\n",
    "print(\"Let's look at the vector for \\\"chocolate\\\" in the cleaned-data model:\\n\")\n",
    "print(mc2.get_word_vector('chocolate'))\n",
    "print(\"\\nWe can also look at the most similar words, or the vector's 'neighbors.' These are determined by the cosine similarity of vectors.\")\n",
    "print(mc2.get_nearest_neighbors('chocolate'))\n",
    "\n",
    "print(\"\\nFasttext also lets us try out analogies. Let's see how it does with the following:\")\n",
    "print(\"\\n\\\"\\'hot\\' is to \\'cold\\', what \\'good\\' is to _____\\\"\")\n",
    "print(mc2.get_analogies('hot','cold','good'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of classifier with dim=50 and trained on cleaned data:\n",
      "0.5907767545364189\n",
      "563\n",
      "563\n",
      "(('__label__5',), array([0.73584759]))\n"
     ]
    }
   ],
   "source": [
    "# Now let's try exporting our vectors and using the sci kit logistic regression model!\n",
    "import get_accuracy\n",
    "\n",
    "\n",
    "'''\n",
    "  FINALLY!\n",
    "  This may have been unecessarily complicated, but now we can compute accuracy with get_accuracy()!\n",
    "'''\n",
    "print(\"Accuracy of classifier with dim=50 and trained on cleaned data:\")\n",
    "print(get_accuracy.get_accuracy('mc1'))\n",
    "\n",
    "print(len(mc2.words))\n",
    "print(len(mc2.words))\n",
    "\n",
    "print(mc2.predict(\"Apples and Bananas\"))\n",
    "\n",
    "# maybe here we can try out some vector addition/subtraction to see what kind of meaning the model captures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}