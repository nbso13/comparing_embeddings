{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitd0f716ec3ba9485fbb6e044caca2f77a",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import pandas as pd\n",
    "import fasttext\n",
    "import fasttext_models\n",
    "\n",
    "execfile('fasttext_models.py')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'execfile' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-753e03e396ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfasttext_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mexecfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fasttext_models.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'execfile' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fasttext classifiers store words by most to least frequent.\n\nHere are the top 50 words for the model trained on cleaned data:\n['>', '<', 'br', '</s>', 'like', 'good', 'taste', 'flavor', 'one', 'get', 'love', 'product', 'make', 'use', 'coffee', 'great', 'try', 'well', 'food', 'buy', 'tea', 'find', 'would', 'eat', 'dog', 'go', 'really', 'time', 'much', 'amazon', 'order', 'also', 'price', 'bag', 'cup', 'give', 'little', 'even', 'drink', 'say', 'think', 'store', 'day', 'cat', 'add', 'box', 'chocolate', 'treat', 'come', 'first']\n\nHere are the top 50 words for the model trained on uncleaned data:\n['the', 'I', 'and', 'a', 'to', 'of', 'is', 'it', '</s>', 'for', 'in', 'this', 'that', 'my', 'with', 'have', 'but', 'are', 'was', 'not', 'you', '/><br', 'on', 'as', 'like', 'they', 'so', 'be', 'The', 'or', 'at', 'these', 'just', 'them', 'very', 'from', 'one', 'good', 'It', '\"I', 'has', 'can', 'taste', 'will', 'would', 'had', 'all', 'more', 'than', 'when']\n"
     ]
    }
   ],
   "source": [
    "# load the models for evaluation\n",
    "m0 = fasttext.load_model('fasttext_skipgram_cleaned.bin')\n",
    "m1 = fasttext.load_model('fasttext_skipgram_uncleaned.bin')\n",
    "print(\"Fasttext classifiers store words by most to least frequent.\")\n",
    "print(\"\\nHere are the top 50 words for the model trained on cleaned data:\")\n",
    "m0_words = m0.get_words()\n",
    "print(m0_words[:50])\n",
    "print(\"\\nHere are the top 50 words for the model trained on uncleaned data:\")\n",
    "m1_words = m1.get_words()\n",
    "print(m1_words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "We can also measure the accuracy of these classfiers by examining their precision and recall when running on a test set.\n",
      "\n",
      "       Model       N  Precision    Recall\n",
      "0    Cleaned  113691   0.737754  0.737754\n",
      "1  Uncleaned  113691   0.778417  0.778417\n",
      "\n",
      "Remarkably, though we know the two models are different, they have identical precision and recall.\n"
     ]
    }
   ],
   "source": [
    "print(\"We can also measure the accuracy of these classfiers by examining their precision and recall when running on a test set.\\n\")\n",
    "\n",
    "n0,p0,r0 = m0.test('reviews_cleaned.test')\n",
    "n1,p1,r1 = m1.test('reviews_uncleaned.test')\n",
    "d = {'Model':['Cleaned', 'Uncleaned'], 'N':[n0,n1], 'Precision':[p0,p1],'Recall':[r0,r1]}\n",
    "df = pd.DataFrame(data=d)\n",
    "print(df)\n",
    "\n",
    "print(\"\\nRemarkably, though we know the two models are different, they have identical precision and recall.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Now let's take a look at some of the vector embeddings.\nLet's look at the vector for \"chocolate\" in the cleaned-data model:\n\n[-0.02479652  0.00130697  0.0024827  -0.01109868  0.02672165 -0.01601592\n  0.02024821  0.02821331 -0.03821959 -0.02994298 -0.00793434 -0.05219461\n -0.05966956 -0.02902482  0.0145936  -0.00822408  0.0551844   0.04180275\n  0.00351004  0.0683969  -0.01808743 -0.0510674  -0.02598811  0.02973687\n  0.09353215 -0.01456618 -0.04731614 -0.02148609 -0.0252507  -0.00568354\n  0.02244259  0.00751527 -0.00421518  0.0400953   0.03187301  0.05662943\n -0.0196273   0.05481498 -0.05990427  0.0099687   0.04064913 -0.00840833\n -0.04372537 -0.02308569 -0.00385587 -0.01340246 -0.04709338 -0.03202996\n -0.02764771 -0.05866976]\n\nWe can also look at the most similar words, or the vector's 'neighbors.' These are determined by the cosine similarity of vectors.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0.8768860697746277, 'jerky'),\n",
       " (0.8577314615249634, 'chai'),\n",
       " (0.8398478627204895, 'invisible'),\n",
       " (0.822700560092926, '97'),\n",
       " (0.8224048614501953, 'kavli'),\n",
       " (0.8212175369262695, 'soaking'),\n",
       " (0.8192944526672363, 'yor'),\n",
       " (0.8187270760536194, 'nation'),\n",
       " (0.8184553384780884, 'possibilitie'),\n",
       " (0.817771315574646, 'pastey')]"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "print(\"Now let's take a look at some of the vector embeddings.\")\n",
    "print(\"Let's look at the vector for \\\"chocolate\\\" in the cleaned-data model:\\n\")\n",
    "print(m0.get_word_vector('chocolate'))\n",
    "print(\"\\nWe can also look at the most similar words, or the vector's 'neighbors.' These are determined by the cosine similarity of vectors.\")\n",
    "m0.get_nearest_neighbors('chocolate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WOWEEE!\")\n",
    "\n",
    "# maybe here we can try out some vector addition/subtraction to see what kind of meaning the model captures. "
   ]
  }
 ]
}